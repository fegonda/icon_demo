<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>PredNet by coxlab</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">PredNet</h1>
      <h2 class="project-tagline">Code and models accompanying &quot;Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning&quot;</h2>
      <h2 class="project-tagline">Bill Lotter, Gabriel Kreiman, and David Cox (2016)</h2>
      <a href="https://arxiv.org/abs/1605.08104" class="btn">View on arXiv</a>
      <a href="https://github.com/coxlab/prednet" class="btn">View on GitHub</a>
      <a href="https://github.com/coxlab/prednet/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/coxlab/prednet/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>The PredNet is a deep convolutional recurrent neural network inspired by the principles of predictive coding from the neuroscience literature [1, 2].
It is trained for next-frame video prediction with the belief that prediction is an effective objective for unsupervised (or "self-supervised") learning [e.g. 3-11].
The PredNet architecture is illustrated below. An animation of the flow of information in the network can be found <a href="prednet_animation.html">here</a>.</p>

<center><img src="/prednet/plots/prednet_model.png"</img></center>

<br>

<p>
Next frame predictions on the Caltech Pedestrian [12] dataset are shown below.
The model was trained on the KITTI dataset [13].
See the repo for downloading the model.</p>
<center><img src="/prednet/plots/caltech_montage_1.gif"</img></center>
<br>
<center><img src="/prednet/plots/caltech_montage_2.gif"</img></center>
<br>
<center><img src="/prednet/plots/caltech_montage_3.gif"</img></center>
<br>

<p>
Multi-timestep ahead predictions can be made by recursively feeding predictions back into the model. Below are several examples for a PredNet
 model fine-tuned for this task.</p>
 <center><img src="/prednet/plots/caltech_montage_extrap_1.gif" width="832"</img></center>

<br>
<br>

<ol>
<li>R. P. N. Rao and D. H. Ballard. Predictive coding in the visual cortex: a functional interpretation
of some extra-classical receptive-field effects. <em>Nature Neuroscience</em>, 1999.</li>
<li>K. Friston. A theory of cortical responses. <em>Philos Trans R Soc Lond B Biol Sci</em>, 2005.</li>
<li>W. Softky. Unsupervised pixel-prediction. <em>NIPS</em>, 1996.</li>
<li>D. George and J. Hawkins.  A hierarchical bayesian model of invariant pattern recognition in
the visual cortex. <em>IJCNN</em>, 2005.</li>
<li>R. B. Palm. Prediction as a candidate for learning deep hierarchical models of data. <em>Master’s
thesis, Technical University of Denmark</em>, 2012.</li>
<li>R. C. O’Reilly, D. Wyatte, and J. Rohrlich. Learning through time in the thalamocortical loops.
<em>arXiv</em>, 2014.</li>
<li>N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations
using lstms. arXiv, 2015.</li>
<li>R. Goroshin, M. Mathieu, and Y. LeCun. Learning to linearize under uncertainty. <em>arXiv</em>, 2015.</li>
<li>M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square
error. <em>arXiv</em>, 2015.</li>
<li>W. Lotter, G. Kreiman, and D. Cox. Unsupervised learning of visual structure using predictive
generative networks. <em>arXiv</em>, 2015.</li>
<li>V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal video autoencoder with differentiable
memory. <em>arXiv</em>, 2015.</li>
<li>P. Dollár, C. Wojek, B. Schiele, and P. Perona. Pedestrian detection: A benchmark. <em>CVPR</em>, 2009.</li>
<li>A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset.
<em>IJRR</em>, 2013.</li>
</ol>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/coxlab/prednet">PredNet</a> is maintained by <a href="https://github.com/coxlab">coxlab</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>


  </body>
</html>
